{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a00745c1-b35d-4bae-bc36-ac5d9ba761dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"3\"\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import numpy as np\n",
    "from utils_analysis import *\n",
    "from PIL import Image\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import utils\n",
    "import vision_transformer as vits\n",
    "from vision_transformer import DINOHead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b68fe15c-5b24-4a1a-9e92-9ef9617a1dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_dino_official_load(arch_name, patch_size, \n",
    "                             models_dir=\"../trained_model_weights/dino_official/backbone/\"):\n",
    "    # getting data file path\n",
    "    weight_path = os.path.join(models_dir, f\"dino_{arch_name}{patch_size}_pretrain.pth\")\n",
    "    checkpoint = torch.load(weight_path, map_location=\"cpu\")\n",
    "\n",
    "    if arch_name == \"deitsmall\":\n",
    "        arch = \"vit_small\"\n",
    "    elif arch_name == \"vitbase\":\n",
    "        arch = \"vit_base\"\n",
    "\n",
    "    model = vits.__dict__[arch](patch_size=patch_size)\n",
    "    model.load_state_dict(checkpoint, strict=True)\n",
    "        \n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01dacc30-c958-4a58-b361-f42e53365cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_deit_official_load(arch=\"vit_small\", patch_size=16):\n",
    "    weight_path = \"../trained_model_weights/deit_official/deit_small_patch16_224.pth\"\n",
    "    checkpoint = torch.load(weight_path, map_location=\"cpu\")\n",
    "    model = vits.__dict__[arch](patch_size=patch_size)\n",
    "    model.load_state_dict(checkpoint[\"model\"], strict=False) # ignore head weights\n",
    "    \n",
    "    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    return model, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "758f5552-3fff-4782-9250-7b70bf2e3885",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir = \"../dataset/Nakano_etal_2010/video_stimuli/frames\"\n",
    "#dataset_dir = \"../dataset/Nakano_etal_2010/video_stimuli/frames_cropped\"\n",
    "border_info = np.load(\"../dataset/Nakano_etal_2010/video_stimuli/border_info.npz\", allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddbb4c53-71a9-462a-9f55-2d4137cee1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_range = border_info['crop_range'] \n",
    "datah = border_info['datah']\n",
    "dataw = border_info['dataw']\n",
    "\n",
    "num_crops = len(crop_range)\n",
    "num_frames = 2327"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a0e2b1f-f9f6-4942-b726-290d882df21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gazepos_official_model_remove_border(model, device, transform, patch_size=16, batch_size=16):\n",
    "    blur_size = patch_size * 2\n",
    "    num_heads = model.num_heads\n",
    "    depth = model.depth\n",
    "    gaze_pos_model_all = np.nan * np.ones((depth, num_heads+1, num_frames, 2))\n",
    "    for crop_idx in tqdm(range(num_crops)):\n",
    "        # load images\n",
    "        s, e = crop_range[crop_idx] # start & end\n",
    "        imgs = [Image.open(dataset_dir + \"/frame{0:04d}.png\".format(frame_idx)) for frame_idx in range(s, e+1)]\n",
    "        images = torch.stack([transform(img) for img in imgs])\n",
    "        images_split = torch.split(images, batch_size)\n",
    "\n",
    "        gaze_pos_model = []\n",
    "        for images_split_part in images_split:\n",
    "            gaze_pos_model_part = get_gaze_pos_model(model, device, images_split_part, patch_size, blur_size)\n",
    "            gaze_pos_model.append(gaze_pos_model_part)\n",
    "        gaze_pos_model = np.concatenate(gaze_pos_model, axis=2)\n",
    "\n",
    "        # modification of offset\n",
    "        gaze_pos_model[:, :, :, 0] += dataw[crop_idx][0]\n",
    "        gaze_pos_model[:, :, :, 1] += datah[crop_idx][0]\n",
    "        gaze_pos_model_all[:, :, (s-1):e] = gaze_pos_model\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    return gaze_pos_model_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fbbe2eb-38eb-49d2-95e1-9d00ddca1d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gazepos_official_model(model, device, transform, patch_size=16, batch_size=32):\n",
    "    blur_size = patch_size * 2\n",
    "    num_heads = model.num_heads\n",
    "    depth = model.depth\n",
    "\n",
    "    image_path_list = sorted(glob.glob(f\"{dataset_dir}/*.png\"))\n",
    "    #assert len(image_path_list) == num_frame, \"num frame is not matched.\"\n",
    "    dataset = ImageDataset(image_path_list, transform)\n",
    "    dataloader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)\n",
    "    gaze_pos_model = get_gaze_pos_model_dataset(model, device, dataloader, patch_size, blur_size)\n",
    "    return gaze_pos_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38a6163d-f0a9-4c41-98be-91aadcc23832",
   "metadata": {},
   "outputs": [],
   "source": [
    "#arch_names = [\"vitbase\"]#[\"deitsmall\", \"vitbase\"]\n",
    "#arch_name = \"vitbase\" #\"deitsmall\", \"vitbase\"\n",
    "#patch_sizes = [8, 16]\n",
    "\n",
    "transform = pth_transforms.Compose([\n",
    "    pth_transforms.ToTensor(),\n",
    "    pth_transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "881f7f31-d90f-4ddb-903b-0fd6bec57ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = \"../dataset/Nakano_etal_2010/preprocessed_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f10d965c-d2d4-4cc3-82ac-5ea334652d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_border = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbfb169e-1b9a-4e8f-9171-fa19854e886f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fbde76b2ea4426582a0dc0080553ac0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "res_dict = {}\n",
    "res_dict[\"info\"] = [\"depth\", \"num_head+mean\", \"num_frames\", \"xy\"]\n",
    "model, device = model_dino_official_load(arch_name=\"deitsmall\", patch_size=16)\n",
    "if remove_border:\n",
    "    res_dict[f\"dino_deit_small16\"] = get_gazepos_official_model_remove_border(model, device, transform)\n",
    "else:\n",
    "    res_dict[f\"dino_deit_small16\"] = get_gazepos_official_model(model, device, transform)\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5249180b-1f88-4bd8-a1cd-ff8cfdfd2337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e04ae5c007564faeb3434d666e19d8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/73 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, device = model_deit_official_load()\n",
    "if remove_border:\n",
    "    res_dict[f\"supervised_deit_small16\"] = get_gazepos_official_model_remove_border(model, device, transform)\n",
    "else:\n",
    "    res_dict[f\"supervised_deit_small16\"] = get_gazepos_official_model(model, device, transform)\n",
    "del model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f9186-b1ea-4a87-94d6-1c95045da51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(f\"{save_dir}/vit_official_gaze_pos.npz\", **res_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5149d4-85d5-49b5-9642-57e6939f8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "batch_size = 2\n",
    "res_dict = {}\n",
    "res_dict[\"info\"] = [\"depth\", \"num_head+mean\", \"num_frames\", \"xy\"]\n",
    "#for arch_name in arch_names:\n",
    "for patch_size in patch_sizes:\n",
    "    blur_size = patch_size * 2\n",
    "    print(arch_name, patch_size)\n",
    "    model, device = model_official_load(arch_name, patch_size)\n",
    "    num_heads = model.num_heads\n",
    "    depth = model.depth\n",
    "    gaze_pos_model_all = np.nan * np.ones((depth, num_heads+1, num_frames, 2))\n",
    "    for crop_idx in tqdm(range(num_crops)):\n",
    "        # load images\n",
    "        s, e = crop_range[crop_idx] # start & end\n",
    "        imgs = [Image.open(dataset_dir + \"/frame{0:04d}.png\".format(frame_idx)) for frame_idx in range(s, e+1)]\n",
    "        images = torch.stack([transform(img) for img in imgs])\n",
    "        images_split = torch.split(images, batch_size)\n",
    "\n",
    "        gaze_pos_model = []\n",
    "        for images_split_part in images_split:\n",
    "            gaze_pos_model_part = get_gaze_pos_model(model, device, images_split_part, patch_size, blur_size)\n",
    "            gaze_pos_model.append(gaze_pos_model_part)\n",
    "        gaze_pos_model = np.concatenate(gaze_pos_model, axis=2)\n",
    "\n",
    "        # modification of offset\n",
    "        gaze_pos_model[:, :, :, 0] += dataw[crop_idx][0]\n",
    "        gaze_pos_model[:, :, :, 1] += datah[crop_idx][0]\n",
    "        gaze_pos_model_all[:, :, (s-1):e] = gaze_pos_model\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    res_dict[f\"{arch_name}{patch_size}\"] = gaze_pos_model_all\n",
    "\n",
    "np.savez_compressed(f\"{save_dir}/vit_official_{arch_name}_gaze_pos.npz\", **res_dict)\n",
    "res_dict.keys()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc00119-153f-4db0-b44b-a06e74168b38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
